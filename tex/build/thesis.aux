\relax 
\providecommand\hyper@newdestlabel[2]{}
\providecommand{\transparent@use}[1]{}
\providecommand\HyperFirstAtBeginDocument{\AtBeginDocument}
\HyperFirstAtBeginDocument{\ifx\hyper@anchor\@undefined
\global\let\oldcontentsline\contentsline
\gdef\contentsline#1#2#3#4{\oldcontentsline{#1}{#2}{#3}}
\global\let\oldnewlabel\newlabel
\gdef\newlabel#1#2{\newlabelxx{#1}#2}
\gdef\newlabelxx#1#2#3#4#5#6{\oldnewlabel{#1}{{#2}{#3}}}
\AtEndDocument{\ifx\hyper@anchor\@undefined
\let\contentsline\oldcontentsline
\let\newlabel\oldnewlabel
\fi}
\fi}
\global\let\hyper@last\relax 
\gdef\HyperFirstAtBeginDocument#1{#1}
\providecommand\HyField@AuxAddToFields[1]{}
\providecommand\HyField@AuxAddToCoFields[2]{}
\babel@aux{english}{}
\citation{keskar2016on}
\citation{hawkins2004the}
\citation{mccloskey1989catastrophic}
\citation{genzel2014light}
\citation{mcclelland2020integration}
\citation{saxe2019a}
\citation{mcclelland2020integration}
\citation{rogers2004semantic}
\citation{mccloskey1989catastrophic}
\citation{ratcliff1990connectionist}
\citation{keskar2016on}
\citation{mcclelland2020integration}
\@writefile{toc}{\contentsline {section}{\numberline {1}Introduction}{3}{section.1}}
\@writefile{toc}{\contentsline {section}{\numberline {2}Cognitive neuroscience}{3}{section.2}}
\@writefile{toc}{\contentsline {subsection}{\numberline {2.1}Catastrophic interference}{3}{subsection.2.1}}
\citation{rumelhart1986a}
\citation{rumelhart1988learning}
\citation{mccloskey1989catastrophic}
\citation{mcclelland1995why}
\citation{ji2007coordinated}
\citation{tang2017hippocampal}
\citation{genzel2014light}
\citation{genzel2014light}
\citation{fosse2003dreaming}
\citation{schaul2016prioritized}
\citation{ven2020brain}
\@writefile{toc}{\contentsline {subsection}{\numberline {2.2}Complementary learning systems}{4}{subsection.2.2}}
\@writefile{toc}{\contentsline {subsection}{\numberline {2.3}Replay}{4}{subsection.2.3}}
\citation{Goodfellow-et-al-2016}
\citation{mesnil2012unsupervised}
\citation{Goodfellow-et-al-2016}
\@writefile{toc}{\contentsline {section}{\numberline {3}Machine learning}{5}{section.3}}
\@writefile{toc}{\contentsline {subsection}{\numberline {3.1}Artificial neural networks}{5}{subsection.3.1}}
\@writefile{toc}{\contentsline {subsection}{\numberline {3.2}Unsupervised learning}{5}{subsection.3.2}}
\@writefile{toc}{\contentsline {subsection}{\numberline {3.3}Denoising autoencoders}{5}{subsection.3.3}}
\citation{strogatz1994nonlinear}
\citation{hinton1987learning}
\@writefile{lof}{\contentsline {figure}{\numberline {1}{\ignorespaces Basic autoencoder network architecture with one hidden layer.\relax }}{6}{figure.caption.2}}
\providecommand*\caption@xref[2]{\@setref\relax\@undefined{#1}}
\newlabel{fig:model}{{1}{6}{Basic autoencoder network architecture with one hidden layer.\relax }{figure.caption.2}{}}
\@writefile{toc}{\contentsline {section}{\numberline {4}Dynamical systems}{6}{section.4}}
\@writefile{toc}{\contentsline {subsection}{\numberline {4.1}Iterative fixed points}{6}{subsection.4.1}}
\citation{strogatz1994nonlinear}
\citation{paszke2019pytorch}
\citation{rogers2004semantic}
\newlabel{fig:fixed:canary}{{2a}{7}{Subfigure 2a}{subfigure.2.1}{}}
\newlabel{sub@fig:fixed:canary}{{(a)}{a}{Subfigure 2a\relax }{subfigure.2.1}{}}
\newlabel{fig:fixed:pine}{{2b}{7}{Subfigure 2b}{subfigure.2.2}{}}
\newlabel{sub@fig:fixed:pine}{{(b)}{b}{Subfigure 2b\relax }{subfigure.2.2}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {2}{\ignorespaces In this particular instance of an underparameterized model, the \textit  {Canary} item is stored as an iterative fixed point, while \textit  {Pine} is not.\relax }}{7}{figure.caption.3}}
\newlabel{fig:fixed}{{2}{7}{In this particular instance of an underparameterized model, the \textit {Canary} item is stored as an iterative fixed point, while \textit {Pine} is not.\relax }{figure.caption.3}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {4.2}Attractors}{7}{subsection.4.2}}
\@writefile{toc}{\contentsline {section}{\numberline {5}Methods}{7}{section.5}}
\citation{rogers2004semantic}
\citation{rogers2004semantic}
\citation{saxe2019a}
\newlabel{fig:data:hierarchy}{{3a}{8}{Subfigure 3a}{subfigure.3.1}{}}
\newlabel{sub@fig:data:hierarchy}{{(a)}{a}{Subfigure 3a\relax }{subfigure.3.1}{}}
\newlabel{fig:data:correlation_matrix}{{3b}{8}{Subfigure 3b}{subfigure.3.2}{}}
\newlabel{sub@fig:data:correlation_matrix}{{(b)}{b}{Subfigure 3b\relax }{subfigure.3.2}{}}
\newlabel{fig:data:dataset}{{3c}{8}{Subfigure 3c}{subfigure.3.3}{}}
\newlabel{sub@fig:data:dataset}{{(c)}{c}{Subfigure 3c\relax }{subfigure.3.3}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {3}{\ignorespaces Category structure in the used hierarchical dataset adapted from \cite  {rogers2004semantic}. The dataset has a shape of $(8,\ 9)$, and contains multiple levels of hierarchy as is visible in the correlation matrix.\relax }}{8}{figure.caption.4}}
\newlabel{fig:data}{{3}{8}{Category structure in the used hierarchical dataset adapted from \cite {rogers2004semantic}. The dataset has a shape of $(8,\ 9)$, and contains multiple levels of hierarchy as is visible in the correlation matrix.\relax }{figure.caption.4}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {4}{\ignorespaces Singular value decomposition of a subset of the training data. Technically speaking, this is the \textit  {thin SVD}, where the matrices $\bm  {U}$ and $\bm  {V}$ contain only the first $k = min(m,\ n)$ columns (the input-output correlation matrix $\bm  {\Sigma }^{yx}$ is $m\times n$). The columns of $\bm  {U}$ and rows of $\bm  {V}$ each form an orthonormal basis. \textit  {Note:} in future references, the singular value matrix here shown as $\bm  {S}$ is instead called $\bm  {\Sigma }$.\relax }}{8}{figure.caption.5}}
\newlabel{fig:svd}{{4}{8}{Singular value decomposition of a subset of the training data. Technically speaking, this is the \textit {thin SVD}, where the matrices $\bm {U}$ and $\bm {V}$ contain only the first $k = min(m,\ n)$ columns (the input-output correlation matrix $\bm {\Sigma }^{yx}$ is $m\times n$). The columns of $\bm {U}$ and rows of $\bm {V}$ each form an orthonormal basis. \textit {Note:} in future references, the singular value matrix here shown as $\bm {S}$ is instead called $\bm {\Sigma }$.\relax }{figure.caption.5}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {5.1}Dataset}{8}{subsection.5.1}}
\citation{dropout}
\citation{batch_norm}
\@writefile{toc}{\contentsline {subsection}{\numberline {5.2}Experimental setup}{9}{subsection.5.2}}
\@writefile{toc}{\contentsline {subsection}{\numberline {5.3}Pattern generation}{9}{subsection.5.3}}
\@writefile{loa}{\contentsline {algorithm}{\numberline {1}{\ignorespaces The algorithm used for pattern generation.\relax }}{10}{algorithm.1}}
\newlabel{alg:pattern}{{1}{10}{The algorithm used for pattern generation.\relax }{algorithm.1}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {5.4}Reliability and limitations}{10}{subsection.5.4}}
\newlabel{subsec:limitations}{{5.4}{10}{Reliability and limitations}{subsection.5.4}{}}
\newlabel{fig:noise_surf}{{5a}{11}{Subfigure 5a}{subfigure.5.1}{}}
\newlabel{sub@fig:noise_surf}{{(a)}{a}{Subfigure 5a\relax }{subfigure.5.1}{}}
\newlabel{fig:resc_corr}{{5b}{11}{Subfigure 5b}{subfigure.5.2}{}}
\newlabel{sub@fig:resc_corr}{{(b)}{b}{Subfigure 5b\relax }{subfigure.5.2}{}}
\newlabel{fig:transition_matrix}{{5c}{11}{Subfigure 5c}{subfigure.5.3}{}}
\newlabel{sub@fig:transition_matrix}{{(c)}{c}{Subfigure 5c\relax }{subfigure.5.3}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {5}{\ignorespaces The existence of a global minimum for $\mu $ and $\sigma $ in the categorical dissimilarity between $\mathbf  {A}$ and $\mathbf  {S}$, and the corresponding transition matrix. $\bm  {A}$ was produced by iterating the trained model according to algorithm \ref  {alg:pattern} for $10,000$ runs, and $\bm  {S}$ was made by using min-max scaling to rescale the correlation matrix to the interval $[0,\ 1]$.\relax }}{11}{figure.caption.6}}
\@writefile{toc}{\contentsline {section}{\numberline {6}Results}{11}{section.6}}
\@writefile{toc}{\contentsline {subsection}{\numberline {6.1}Underparameterized scenario}{11}{subsection.6.1}}
\@writefile{toc}{\contentsline {subsection}{\numberline {6.2}Overparameterized scenario}{11}{subsection.6.2}}
\newlabel{fig:under:loss}{{6a}{12}{Subfigure 6a}{subfigure.6.1}{}}
\newlabel{sub@fig:under:loss}{{(a)}{a}{Subfigure 6a\relax }{subfigure.6.1}{}}
\newlabel{fig:under:loss_per_item}{{6b}{12}{Subfigure 6b}{subfigure.6.2}{}}
\newlabel{sub@fig:under:loss_per_item}{{(b)}{b}{Subfigure 6b\relax }{subfigure.6.2}{}}
\newlabel{fig:under:singular}{{6c}{12}{Subfigure 6c}{subfigure.6.3}{}}
\newlabel{sub@fig:under:singular}{{(c)}{c}{Subfigure 6c\relax }{subfigure.6.3}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {6}{\ignorespaces Training metrics for an \textit  {underparameterized} first model with a hidden size of 2, averaged over 25 model runs. Because of limited capacity, the network is forced to generalize, which is visible in the large deviations in individual item losses between model runs, and also in the small amount of nonzero singular values.\relax }}{12}{figure.caption.7}}
\newlabel{fig:under}{{6}{12}{Training metrics for an \textit {underparameterized} first model with a hidden size of 2, averaged over 25 model runs. Because of limited capacity, the network is forced to generalize, which is visible in the large deviations in individual item losses between model runs, and also in the small amount of nonzero singular values.\relax }{figure.caption.7}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {7}{\ignorespaces Visualization of the latent space learned by an underparameterized first model. Not every item is stored as an attractor, and some representations heavily overlap. This plot was produced by generating a grid of $10,000$ $2d$ points and feeding it only through the decoder part of the network, after which they were categorized as the item that was closest (measured by Euclidean distance) to the model prediction and colored appropriately. The arrows stem from iterating each point for $25$ steps and computing the difference between start and end states in the hidden layer; their magnitudes are proportional to the distance covered. The stars represent the hidden encodings of the items $\bm  {x} \in \mathcal  {X}$.\relax }}{12}{figure.caption.8}}
\newlabel{fig:singlet_attr}{{7}{12}{Visualization of the latent space learned by an underparameterized first model. Not every item is stored as an attractor, and some representations heavily overlap. This plot was produced by generating a grid of $10,000$ $2d$ points and feeding it only through the decoder part of the network, after which they were categorized as the item that was closest (measured by Euclidean distance) to the model prediction and colored appropriately. The arrows stem from iterating each point for $25$ steps and computing the difference between start and end states in the hidden layer; their magnitudes are proportional to the distance covered. The stars represent the hidden encodings of the items $\bm {x} \in \mathcal {X}$.\relax }{figure.caption.8}{}}
\newlabel{fig:under_over_sample:nonoise}{{8a}{12}{Subfigure 8a}{subfigure.8.1}{}}
\newlabel{sub@fig:under_over_sample:nonoise}{{(a)}{a}{Subfigure 8a\relax }{subfigure.8.1}{}}
\newlabel{fig:under_over_sample:noise}{{8b}{12}{Subfigure 8b}{subfigure.8.2}{}}
\newlabel{sub@fig:under_over_sample:noise}{{(b)}{b}{Subfigure 8b\relax }{subfigure.8.2}{}}
\newlabel{fig:under_over_sample:rel_inc}{{8c}{12}{Subfigure 8c}{subfigure.8.3}{}}
\newlabel{sub@fig:under_over_sample:rel_inc}{{(c)}{c}{Subfigure 8c\relax }{subfigure.8.3}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {8}{\ignorespaces The addition of noise during the sampling process increases the prevalence of certain items in the replay dataset, causing them to be learned better at the expense of other items.\relax }}{12}{figure.caption.9}}
\newlabel{fig:under_over_sample}{{8}{12}{The addition of noise during the sampling process increases the prevalence of certain items in the replay dataset, causing them to be learned better at the expense of other items.\relax }{figure.caption.9}{}}
\newlabel{fig:over:loss}{{9a}{13}{Subfigure 9a}{subfigure.9.1}{}}
\newlabel{sub@fig:over:loss}{{(a)}{a}{Subfigure 9a\relax }{subfigure.9.1}{}}
\newlabel{fig:over:loss_per_item}{{9b}{13}{Subfigure 9b}{subfigure.9.2}{}}
\newlabel{sub@fig:over:loss_per_item}{{(b)}{b}{Subfigure 9b\relax }{subfigure.9.2}{}}
\newlabel{fig:over:singular}{{9c}{13}{Subfigure 9c}{subfigure.9.3}{}}
\newlabel{sub@fig:over:singular}{{(c)}{c}{Subfigure 9c\relax }{subfigure.9.3}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {9}{\ignorespaces Training metrics for an \textit  {overparameterized} first model with a hidden size of 32. In a short amount of time, all items are able to approach a loss of 0.\relax }}{13}{figure.caption.10}}
\newlabel{fig:over}{{9}{13}{Training metrics for an \textit {overparameterized} first model with a hidden size of 32. In a short amount of time, all items are able to approach a loss of 0.\relax }{figure.caption.10}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {10}{\ignorespaces The existence of iterative fixed points and their basins of attraction in the latent space of an overparameterized model with a hidden size of 32. The explained variance by the PCA dimensions were $0.58$, $0.21$ and $0.12$ respectively, adding up to $0.91$. Note that some stars (representing the items) lie in areas of a different color; this is due to the grid points being passed through the \textit  {decoder} and colored based on the prediction, whereas the items were only passed through the \textit  {encoder} to get their position in the latent space. Passing the point corresponding to the item through the decoder too would classify it according to the background color of the points around it.\relax }}{13}{figure.caption.11}}
\newlabel{fig:triplet_attr}{{10}{13}{The existence of iterative fixed points and their basins of attraction in the latent space of an overparameterized model with a hidden size of 32. The explained variance by the PCA dimensions were $0.58$, $0.21$ and $0.12$ respectively, adding up to $0.91$. Note that some stars (representing the items) lie in areas of a different color; this is due to the grid points being passed through the \textit {decoder} and colored based on the prediction, whereas the items were only passed through the \textit {encoder} to get their position in the latent space. Passing the point corresponding to the item through the decoder too would classify it according to the background color of the points around it.\relax }{figure.caption.11}{}}
\@writefile{toc}{\contentsline {section}{\numberline {7}Discussion}{13}{section.7}}
\newlabel{fig:over_over_sample:nonoise}{{11a}{14}{Subfigure 11a}{subfigure.11.1}{}}
\newlabel{sub@fig:over_over_sample:nonoise}{{(a)}{a}{Subfigure 11a\relax }{subfigure.11.1}{}}
\newlabel{fig:over_over_sample:noise}{{11b}{14}{Subfigure 11b}{subfigure.11.2}{}}
\newlabel{sub@fig:over_over_sample:noise}{{(b)}{b}{Subfigure 11b\relax }{subfigure.11.2}{}}
\newlabel{fig:over_over_sample:rel_inc}{{11c}{14}{Subfigure 11c}{subfigure.11.3}{}}
\newlabel{sub@fig:over_over_sample:rel_inc}{{(c)}{c}{Subfigure 11c\relax }{subfigure.11.3}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {11}{\ignorespaces Even when the first model is overcomplete, a second model can occasionally fail to learn specific items when trained on sampled patterns. The addition of noise can improve the general performance of the second model.\relax }}{14}{figure.caption.12}}
\newlabel{fig:over_over_sample}{{11}{14}{Even when the first model is overcomplete, a second model can occasionally fail to learn specific items when trained on sampled patterns. The addition of noise can improve the general performance of the second model.\relax }{figure.caption.12}{}}
\@writefile{toc}{\contentsline {section}{\numberline {8}Conclusions}{14}{section.8}}
\citation{saxe2019a}
\@writefile{toc}{\contentsline {section}{\numberline {9}Recommendations}{15}{section.9}}
\bibstyle{plain}
\bibdata{bibliography}
\bibcite{fosse2003dreaming}{1}
\bibcite{genzel2014light}{2}
\bibcite{Goodfellow-et-al-2016}{3}
\bibcite{hawkins2004the}{4}
\bibcite{hinton1987learning}{5}
\bibcite{batch_norm}{6}
\bibcite{ji2007coordinated}{7}
\bibcite{keskar2016on}{8}
\bibcite{mcclelland2020integration}{9}
\bibcite{mcclelland1995why}{10}
\bibcite{mccloskey1989catastrophic}{11}
\bibcite{mesnil2012unsupervised}{12}
\bibcite{paszke2019pytorch}{13}
\bibcite{ratcliff1990connectionist}{14}
\bibcite{rogers2004semantic}{15}
\bibcite{rumelhart1986a}{16}
\bibcite{rumelhart1988learning}{17}
\bibcite{saxe2019a}{18}
\bibcite{schaul2016prioritized}{19}
\bibcite{dropout}{20}
\bibcite{strogatz1994nonlinear}{21}
\bibcite{tang2017hippocampal}{22}
\bibcite{ven2020brain}{23}
